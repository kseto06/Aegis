{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclist Detection\n",
    "\n",
    "This notebook will finetune models using Ultralytics YOLOv11 to detect cyclists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (8.3.85)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (3.9.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (0.17.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (4.67.0)\n",
      "Requirement already satisfied: psutil in /Users/kaden/Library/Python/3.12/lib/python/site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Tuple, Type\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import colorsys\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import imghdr\n",
    "\n",
    "%pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#Webcam:\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEGACY: Data Processor\n",
    "\n",
    "Using Ultralytics YAML training extension to reduce RAM usage from loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(Dataset):\n",
    "    def __init__(self, image_dir: str = 'dataset/images', label_dir: str = 'dataset/labels'):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.jpg')]\n",
    "        self.label_paths = [os.path.join(label_dir, fname) for fname in os.listdir(label_dir) if fname.endswith('.txt')]\n",
    "        \n",
    "        # Transform the images to tensors:\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.images, self.labels = self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def load_data(self) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        # Iterate over each image and label file\n",
    "        for img_path, label_path in zip(self.image_paths, self.label_paths):\n",
    "    \n",
    "            # Loading images\n",
    "            try:\n",
    "                image = self.transform(Image.open(img_path).convert('RGB'))\n",
    "                self.images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f'Error opening image: {img_path}')\n",
    "                continue\n",
    "            \n",
    "            # Loading labels/bounding boxes\n",
    "            bounding_boxes = []\n",
    "            try:\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        # Iterate over each line in the label file\n",
    "                        label: list[str] = line.strip().split()\n",
    "                        if len(label) == 5:\n",
    "                            try:\n",
    "                                # Compute each bounding box\n",
    "                                class_id = int(label[0])\n",
    "                                bbox_values = [float(x) for x in label[1:]]\n",
    "                                bounding_box = torch.tensor([class_id+1] + bbox_values, dtype=torch.float32) #class_id = 1 corresponds to cyclist in Ultralytics YOLO\n",
    "                                bounding_boxes.append(bounding_box)\n",
    "                            except ValueError:\n",
    "                                print(f\"invalid value in label line: {line} in {label_path}\")\n",
    "                \n",
    "                # Stack the tensors of bounding boxes\n",
    "                if bounding_boxes:\n",
    "                    labels = torch.stack(bounding_boxes)\n",
    "                else:\n",
    "                    labels = torch.empty((0, 5), dtype=torch.float32)\n",
    "            except Exception as e:\n",
    "                print(f'Error opening label: {label_path}')\n",
    "                continue\n",
    "            \n",
    "            self.labels.append(labels)\n",
    "        \n",
    "        return self.images, self.labels\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "    def split_data(self, test_size: float = 0.2, random_state: int = 42):\n",
    "        self.train_images, self.val_images, self.train_labels, self.val_labels = train_test_split(\n",
    "            self.images, self.labels, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "    def get_train_data(self) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        return self.train_images, self.train_labels\n",
    "    \n",
    "    def get_val_data(self) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        return self.val_images, self.val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_Detection():\n",
    "    def __init__(self, model_path: str='yolo/yolo11s.pt'):\n",
    "        self.CLASSES: list[str] = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "           'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "           'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "           'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "           'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "           'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "           'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "           'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "           'hair drier', 'toothbrush']\n",
    "        \n",
    "        self.model = YOLO(model_path)\n",
    "\n",
    "    def filter_boxes(self, box_confidence: torch.Tensor, boxes: torch.Tensor, box_class_probs: torch.Tensor, threshold: float = .6) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        This function filters boxes using confidence and class probabilities and seeing if they lie above the certain threshold.\n",
    "        '''\n",
    "\n",
    "        # Compute the score of a box as the confidence that there's some object * the probability of it being in a certain class\n",
    "        box_scores = box_confidence * box_class_probs\n",
    "\n",
    "        box_classes = torch.argmax(box_scores, dim=-1)\n",
    "        box_class_scores, _ = torch.max(box_scores, dim=-1, keepdim=False)\n",
    "        filtering_mask = (box_class_scores >= threshold) # Only filter & keep boxes above the threshold\n",
    "\n",
    "        # Convert scores to boolean values using the filtering mask\n",
    "        scores = torch.masked_select(box_class_scores[filtering_mask])\n",
    "        boxes = torch.masked_select(boxes[filtering_mask])\n",
    "        classes = torch.masked_select(box_classes[filtering_mask])\n",
    "\n",
    "        return scores, boxes, classes\n",
    "\n",
    "    def iou(self, box1: Tuple[float, float, float, float], box2: Tuple[float, float, float, float]) -> float:\n",
    "        '''\n",
    "        Design IOU for non-max suppression (NMS) -- we want to use NMS to only select the most accurate (highest probability of the 3 boxes)\n",
    "        '''\n",
    "        (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
    "        (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
    "\n",
    "        # Compute intersections\n",
    "        xi1 = np.maximum(box1[0], box2[0])\n",
    "        yi1 = np.maximum(box1[1], box2[1])\n",
    "        xi2 = np.minimum(box1[2], box2[2])\n",
    "        yi2 = np.minimum(box1[3], box2[3])\n",
    "        intersection_width = xi2 - xi1\n",
    "        intersection_height = yi2 - yi1\n",
    "        intersection_area = max(intersection_width, 0) * max(intersection_height, 0) #Case where areas do not intersect\n",
    "\n",
    "        # Compute Union Area and return the iou\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "        return float(intersection_area) / float(union_area)\n",
    "\n",
    "    def non_max_suppression(self, scores: torch.Tensor, boxes: torch.Tensor, classes: torch.Tensor, max_boxes: int = 10, iou_threshold: float = 0.5) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        '''\n",
    "        Non-max suppression: Select the highest-score box, overlap the box and remove boxes that overlap significantly\n",
    "        '''\n",
    "        nms_detections: list = torch.ops.torchvision.nms(boxes, scores, iou_threshold)\n",
    "        nms_detections = nms_detections[:max_boxes]\n",
    "\n",
    "        return scores[nms_detections], boxes[nms_detections], classes[nms_detections]\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Finetune the pre-trained model on the cyclist dataset using Ultralytics YOLO\n",
    "        '''\n",
    "        # Load the dataset\n",
    "        dataset = DataProcessor()\n",
    "        dataset.split_data()\n",
    "        train_images, train_labels = dataset.get_train_data()\n",
    "        val_images, val_labels = dataset.get_val_data()\n",
    "\n",
    "        # Load the model\n",
    "        train_data = TensorDataset(torch.stack(train_images), torch.stack(train_labels))\n",
    "        val_data = TensorDataset(torch.stack(val_images), torch.stack(val_labels))\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Finetune pre-trained model\n",
    "        self.model.train(data=train_loader, val_data=val_loader, epochs=50, imgsz=640, batch=16)\n",
    "        self.model.export(format='onnx')\n",
    "    \n",
    "    def yolo_train(self):\n",
    "        '''\n",
    "        Finetune the pre-trained model using .yaml file\n",
    "        '''\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.train(data=\"yolo/train.yaml\", epochs=50, imgsz=640, batch=16, device=device)\n",
    "        self.model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEGACY: Inference\n",
    "\n",
    "Refer to the `cyclist-cv.py` file for updated live inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(): \n",
    "    # Pass in a yolo class and model path\n",
    "    def __init__(self, yolo: Type[object], model_path: str = 'yolo/yolo11n.onnx'):\n",
    "        self.model = YOLO(model_path)\n",
    "        self.CLASSES = yolo.CLASSES\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def predict(self, video_src=0, score_threshold=0.6, iou_threshold=0.5, max_boxes=10, use_webcam=False):\n",
    "        if use_webcam:\n",
    "            capture = cv2.VideoCapture(f'http://192.168.205.149:8080/video') #IP when connected to hotspot data\n",
    "        else:\n",
    "            capture = cv2.VideoCapture(video_src)\n",
    "\n",
    "        if not capture.isOpened():\n",
    "            raise Exception(\"Could not open video device\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Run model prediction\n",
    "            prediction = self.model(frame)\n",
    "\n",
    "            # Evaluate the predictions\n",
    "            scores, boxes, classes = self.evaluate(prediction, img_shape=(frame.shape[0], frame.shape[1]), max_boxes=max_boxes, score_threshold=score_threshold, iou_threshold=iou_threshold)\n",
    "\n",
    "            # Draw the bounding boxes\n",
    "            self.draw_boxes(frame, scores, boxes, classes, self.CLASSES, self.generate_colors(self.CLASSES))\n",
    "            cv2.imshow(\"Cyclist Detection\", frame)\n",
    "\n",
    "            if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "                break\n",
    "                \n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Evaluation functions\n",
    "    def evaluate(self, model_output: Tuple[Tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor]], img_shape = (720., 1280.), max_boxes=10, score_threshold = 0.6, iou_threshold = 0.5) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Unpack outputs of the model\n",
    "        box_confidence, boxes, box_class_probs, classes = model_output\n",
    "\n",
    "        # Convert the boxes to the corners\n",
    "        boxes = self.boxes_to_corners(boxes)\n",
    "\n",
    "        # Filter the boxes\n",
    "        scores, boxes, classes = self.filter_boxes(box_confidence, boxes, box_class_probs, threshold=score_threshold)\n",
    "\n",
    "        # Scale boxes to the original image shape\n",
    "        boxes = self.scale_boxes(boxes, img_shape)\n",
    "\n",
    "        # Perform and return non-max suppression\n",
    "        return self.non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)\n",
    "\n",
    "    def boxes_to_corners(boxes: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Helper function to convert YOLO boxes to bounding box corners\n",
    "        '''\n",
    "        x_center, y_center, width, height = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "        x_min = x_center - (width / 2)\n",
    "        y_min = y_center - (height / 2)\n",
    "        x_max = x_center + (width / 2)\n",
    "        y_max = y_center + (height / 2)\n",
    "\n",
    "        return torch.stack([x_min, y_min, x_max, y_max], dim=1)\n",
    "\n",
    "    '''\n",
    "    Helper functions for YOLO inference, drawing on webcam:\n",
    "    '''\n",
    "    def generate_colors(class_names):\n",
    "        '''\n",
    "        Generates random HSV --> RGB colors for each class\n",
    "        '''\n",
    "        hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
    "        colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "        random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        random.seed(None)  # Reset seed to default.\n",
    "        return colors\n",
    "\n",
    "    def scale_boxes(boxes, image_shape):\n",
    "        \"\"\"\n",
    "        Scales the predicted boxes in order to be drawable on the image\n",
    "        \"\"\"\n",
    "        height = image_shape[0]\n",
    "        width = image_shape[1]\n",
    "        image_dims = torch.tensor([height, width, height, width])\n",
    "        image_dims = torch.reshape(image_dims, [1, 4])\n",
    "        boxes = boxes * image_dims\n",
    "        return boxes\n",
    "\n",
    "    def preprocess_frame(frame, model_image_size):\n",
    "        '''\n",
    "        Preprocess frame into data that can be inputted into the model\n",
    "        '''\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        resized_image = image.resize(tuple(reversed(model_image_size)), Image.BICUBIC)\n",
    "        image_data = np.array(resized_image, dtype='float32')\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "        return image_data\n",
    "\n",
    "    def draw_boxes(frame, out_scores, out_boxes, out_classes, class_names, colors):\n",
    "        '''\n",
    "        This function draws the bounding box with class labels/scores over the frame.\n",
    "        '''\n",
    "        thickness = (frame.shape[0] + frame.shape[1]) // 300\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(frame.shape[0], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(frame.shape[1], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), colors[c], thickness)\n",
    "\n",
    "            # Draw label\n",
    "            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            label_top = max(top, label_size[1])\n",
    "            cv2.rectangle(frame, (left, label_top - label_size[1]), (left + label_size[0], label_top + 5), colors[c], cv2.FILLED)\n",
    "            cv2.putText(frame, label, (left, label_top), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "        return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO_Detection()\n",
    "train = True\n",
    "\n",
    "if train:\n",
    "    yolo.yolo_train()\n",
    "\n",
    "# inference = Inference(yolo, model_path='yolo/yolo11n.pt')\n",
    "# inference.predict(video_src=0, score_threshold=0.6, iou_threshold=0.5, max_boxes=10, use_webcam=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
